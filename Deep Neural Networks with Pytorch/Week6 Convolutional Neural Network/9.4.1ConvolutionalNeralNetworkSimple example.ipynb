{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0110ENSkillsNetwork20647811-2022-01-01\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n",
    "<h1 align=center><font size = 5>Convolutional Neral Network Simple example </font></h1> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Objective for this Notebook<h3>    \n",
    "<h5> 1. Learn Convolutional Neral Network</h5>\n",
    "<h5> 2. Define Softmax , Criterion function, Optimizer and Train the  Model</h5>    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents\n",
    "\n",
    "In this lab, we will use a Convolutional Neral Networks to classify horizontal an vertical Lines\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "<li><a href=\"https://#ref0\">Helper functions </a></li>\n",
    "\n",
    "<li><a href=\"https://#ref1\"> Prepare Data </a></li>\n",
    "<li><a href=\"https://#ref2\">Convolutional Neral Network </a></li>\n",
    "<li><a href=\"https://#ref3\">Define Softmax , Criterion function, Optimizer and Train the  Model</a></li>\n",
    "<li><a href=\"https://#ref4\">Analyse Results</a></li>\n",
    "\n",
    "<br>\n",
    "<p></p>\n",
    "Estimated Time Needed: <strong>25 min</strong>\n",
    "</div>\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref0\"></a>\n",
    "\n",
    "<h2 align=center>Helper functions </h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as dsets\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f5b10105b10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function to plot out the parameters of the Convolutional layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_channels(W):\n",
    "    #number of output channels \n",
    "    n_out=W.shape[0]\n",
    "    #number of input channels \n",
    "    n_in=W.shape[1]\n",
    "    w_min=W.min().item()\n",
    "    w_max=W.max().item()\n",
    "    fig, axes = plt.subplots(n_out,n_in)\n",
    "    fig.subplots_adjust(hspace = 0.1)\n",
    "    out_index=0\n",
    "    in_index=0\n",
    "    #plot outputs as rows inputs as columns \n",
    "    for ax in axes.flat:\n",
    "    \n",
    "        if in_index>n_in-1:\n",
    "            out_index=out_index+1\n",
    "            in_index=0\n",
    "              \n",
    "        ax.imshow(W[out_index,in_index,:,:], vmin=w_min, vmax=w_max, cmap='seismic')\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_xticklabels([])\n",
    "        in_index=in_index+1\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>show_data</code>: plot out data sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_data(dataset,sample):\n",
    "\n",
    "    plt.imshow(dataset.x[sample,0,:,:].numpy(),cmap='gray')\n",
    "    plt.title('y='+str(dataset.y[sample].item()))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create some toy data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "class Data(Dataset):\n",
    "    def __init__(self,N_images=100,offset=0,p=0.9, train=False):\n",
    "        \"\"\"\n",
    "        p:portability that pixel is wight  \n",
    "        N_images:number of images \n",
    "        offset:set a random vertical and horizontal offset images by a sample should be less than 3 \n",
    "        \"\"\"\n",
    "        if train==True:\n",
    "            np.random.seed(1)  \n",
    "        \n",
    "        #make images multiple of 3 \n",
    "        N_images=2*(N_images//2)\n",
    "        images=np.zeros((N_images,1,11,11))\n",
    "        start1=3\n",
    "        start2=1\n",
    "        self.y=torch.zeros(N_images).type(torch.long)\n",
    "\n",
    "        for n in range(N_images):\n",
    "            if offset>0:\n",
    "        \n",
    "                low=int(np.random.randint(low=start1, high=start1+offset, size=1))\n",
    "                high=int(np.random.randint(low=start2, high=start2+offset, size=1))\n",
    "            else:\n",
    "                low=4\n",
    "                high=1\n",
    "        \n",
    "            if n<=N_images//2:\n",
    "                self.y[n]=0\n",
    "                images[n,0,high:high+9,low:low+3]= np.random.binomial(1, p, (9,3))\n",
    "            elif  n>N_images//2:\n",
    "                self.y[n]=1\n",
    "                images[n,0,low:low+3,high:high+9] = np.random.binomial(1, p, (3,9))\n",
    "           \n",
    "        \n",
    "        \n",
    "        self.x=torch.from_numpy(images).type(torch.FloatTensor)\n",
    "        self.len=self.x.shape[0]\n",
    "        del(images)\n",
    "        np.random.seed(0)\n",
    "    def __getitem__(self,index):      \n",
    "        return self.x[index],self.y[index]\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<code>plot_activation</code>: plot out the activations of the Convolutional layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_activations(A,number_rows= 1,name=\"\"):\n",
    "    A=A[0,:,:,:].detach().numpy()\n",
    "    n_activations=A.shape[0]\n",
    "    \n",
    "    \n",
    "    print(n_activations)\n",
    "    A_min=A.min().item()\n",
    "    A_max=A.max().item()\n",
    "\n",
    "    if n_activations==1:\n",
    "\n",
    "        # Plot the image.\n",
    "        plt.imshow(A[0,:], vmin=A_min, vmax=A_max, cmap='seismic')\n",
    "\n",
    "    else:\n",
    "        fig, axes = plt.subplots(number_rows, n_activations//number_rows)\n",
    "        fig.subplots_adjust(hspace = 0.4)\n",
    "        for i,ax in enumerate(axes.flat):\n",
    "            if i< n_activations:\n",
    "                # Set the label for the sub-plot.\n",
    "                ax.set_xlabel( \"activation:{0}\".format(i+1))\n",
    "\n",
    "                # Plot the image.\n",
    "                ax.imshow(A[i,:], vmin=A_min, vmax=A_max, cmap='seismic')\n",
    "                ax.set_xticks([])\n",
    "                ax.set_yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility function for computing output of convolutions\n",
    "takes a tuple of (h,w) and returns a tuple of (h,w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def conv_output_shape(h_w, kernel_size=1, stride=1, pad=0, dilation=1):\n",
    "    #by Duane Nielsen\n",
    "    from math import floor\n",
    "    if type(kernel_size) is not tuple:\n",
    "        kernel_size = (kernel_size, kernel_size)\n",
    "    h = floor( ((h_w[0] + (2 * pad) - ( dilation * (kernel_size[0] - 1) ) - 1 )/ stride) + 1)\n",
    "    w = floor( ((h_w[1] + (2 * pad) - ( dilation * (kernel_size[1] - 1) ) - 1 )/ stride) + 1)\n",
    "    return h, w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref1\"></a>\n",
    "\n",
    "<h2 align=center>Prepare Data </h2> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the training dataset with 10000 samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_images=10000\n",
    "train_dataset=Data(N_images=N_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the testing dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Data at 0x7f5a87273090>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_dataset=Data(N_images=1000,train=False)\n",
    "validation_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see the data type is long\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each element in the rectangular  tensor corresponds to a number representing a pixel intensity  as demonstrated by  the following image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print out the third label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAEICAYAAACZA4KlAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAL9klEQVR4nO3df6zddX3H8efLVqYFCSxmCgUFEuLm2BimcSDLYsAlXSTCHzNiwubIkv6xOVG3GWeyuGRZYjKzQJb9SO1wjRCIKcQRY9DFEfWfNZQyBm1xY6hQKT+MCLh/kPDeH/fgLnf3trfnfM/9nvb9fCTNvfdwfrx7b598PufH/Z5UFZJOfq8ZewBJG8PYpSaMXWrC2KUmjF1qwtilJoxdasLYddyS/EySm5M8n+TJJB8feyYd2+axB9AJ6c+BC4G3Am8G7klysKruHnUqHZUrezNJ/iTJHStO+5skNx7H1fwO8BdV9WxVHQI+B/zucFNqHoy9n1uA7UnOAEiyGfgA8IUkf5fkR2v8+Y/J+c8EzgYeWHadDwC/uLF/DR0vt/HNVNWRJN8E3s/Sirwd+EFV3QfcB/z+Ma7itMnH55ad9hzwhqFn1bBc2XvaDVw3+fw64AvHcdkfTz6evuy004EXBphLc2TsPX0J+OUkFwFXAbcCJPmHJD9e488BgKp6FjgCXLzs+i4GDmzsX0HHK/6Ka09JPgf8Kktb+CuO87KfAS4DrgHeBNwDXO+j8YvNlb2v3cAvcXxb+Fd8Gvhv4HvAN4C/MvTF58reVJK3AA8Db66q58eeR/Pnyt5QktcAHwduN/Q+fOqtmSSnAk+xtAXfPvI42kBu46Um3MZLTWzoNj6J2whpzqoqq53uyi41YexSE8YuNWHsUhPGLjVh7FITM8WeZHuSbyd5JMknhxpK0vCmfgVdkk3AfwK/ARwG7gU+WFUHj3IZn2eX5mwez7O/E3ikqh6tqheB24GrZ7g+SXM0S+xbgceXfX14ctqrJNmRZF+SfTPclqQZzfJy2dW2Cv9vm15VO4Gd4DZeGtMsK/th4NxlX58DPDHbOJLmZZbY7wUuTHJ+klOAa4G7hhlL0tCm3sZX1UtJPgx8FdgE3FxVHmFUWlAbevAK77NL8+evuErNGbvUhLFLTRi71ISHkj5BdDsKcLLqY0yagSu71ISxS00Yu9SEsUtNGLvUhLFLTRi71ISxS00Yu9SEsUtNGLvUhLFLTRi71ISxS00Yu9SEsUtNGLvUhLFLTRi71ISxS00Yu9SEsUtNGLvUhLFLTRi71ISxS01MHXuSc5Pck+RQkgNJbhhyMEnDyrTvIZbkLOCsqtqf5A3AfcA1VXXwKJfp9YZlA/K93rReVbXqN2/qlb2qjlTV/snnLwCHgK3TXp+k+RrkXVyTnAdcAuxd5b/tAHYMcTuSpjf1Nv6nV5CcBnwD+MuquvMY5+21Fx2Q23it1+DbeIAkrwXuAG49VuiSxjXLA3QBdgM/rKqPrvMyvZanAbmya73WWtlnif3XgG8BDwIvT07+VFV95SiX6fUvdkDGrvUaPPZpGPv0jF3rNZf77JJOHMYuNWHsUhPGLjVh7FITxi41YexSE8YuNWHsUhPGLjVh7FITxi41YexSE8YuNWHsUhPGLjVh7FITxi41YexSE8YuNWHsUhPGLjVh7FITxi41YexSE8YuNWHsUhPGLjVh7FITxi41MXPsSTYluT/Jl4cYSNJ8DLGy3wAcGuB6JM3RTLEnOQd4L7BrmHEkzcusK/uNwCeAl9c6Q5IdSfYl2TfjbUmawdSxJ7kKeLqq7jva+apqZ1Vtq6pt096WpNnNsrJfDrwvyXeB24ErktwyyFSSBpeqmv1KkncDf1xVVx3jfLPfWFND/JxOJEnGHuGEVVWrfvN8nl1qYpCVfd035so+NVd2rZcru9ScsUtNGLvUhLFLTWweewCdHHxAbfG5sktNGLvUhLFLTRi71ISxS00Yu9SEsUtNGLvUhLFLTRi71ISxS00Yu9SEsUtNGLvUhLFLTRi71ISxS00Yu9SEsUtNeAw6DWLoN7HwmHbDc2WXmjB2qQljl5owdqkJY5eaMHapiZliT3JGkj1JHk5yKMllQw0maVizPs9+E3B3Vf1WklOALQPMJGkOMu2LIZKcDjwAXFDrvJIkw77yopGhX7Sy6HxRzfSqatVv3izb+AuAZ4DPJ7k/ya4kp648U5IdSfYl2TfDbUma0Swr+zbg34DLq2pvkpuA56vqz45ymV7L04Bc2bVe81jZDwOHq2rv5Os9wDtmuD5JczR17FX1JPB4krdNTroSODjIVJIGN/U2HiDJrwC7gFOAR4Hrq+rZo5y/1150QG7jtV5rbeNniv14Gfv0jF3rNY/77JJOIMYuNWHsUhPGLjVh7FITxi41YexSE8YuNWHsUhPGLjVh7FITxi41YexSE8YuNWHsUhPGLjVh7FITxi41YexSE8YuNWHsUhPGLjVh7FITxi41YexSE8YuNWHsUhPGLjVh7FITxi41MVPsST6W5ECSh5LcluR1Qw0maVhTx55kK/ARYFtVXQRsAq4dajBJw5p1G78ZeH2SzcAW4InZR5I0D1PHXlXfBz4LPAYcAZ6rqq+tPF+SHUn2Jdk3/ZiSZjXLNv5M4GrgfOBs4NQk1608X1XtrKptVbVt+jElzWqWbfx7gO9U1TNV9RPgTuBdw4wlaWizxP4YcGmSLUkCXAkcGmYsSUOb5T77XmAPsB94cHJdOweaS9LAUlUbd2PJxt3YSWYjf06LYGmzqGlU1arfPF9BJzVh7FITxi41YexSE5vHHkDjGPoBsG4PIJ6IXNmlJoxdasLYpSaMXWrC2KUmjF1qwtilJoxdasLYpSaMXWrC2KUmjF1qwtilJoxdasLYpSaMXWrC2KUmjF1qwtilJjwG3Qli0d80YdHnkyu71IaxS00Yu9SEsUtNGLvUhLFLTRwz9iQ3J3k6yUPLTvvZJP+S5L8mH8+c75iSZrWelf2fgO0rTvsk8PWquhD4+uRrSQvsmLFX1TeBH644+Wpg9+Tz3cA1w44laWjTvoLuTVV1BKCqjiT5ubXOmGQHsGPK25E0kLm/XLaqdgI7AZL4vr7SSKZ9NP6pJGcBTD4+PdxIkuZh2tjvAj40+fxDwD8PM46keUnV0XfWSW4D3g28EXgK+DTwJeCLwFuAx4D3V9XKB/FWuy638dKcVdWqv4J4zNiHZOzS/K0Vu6+gk5owdqkJY5eaMHapiY0+Bt0PgO+t43xvnJx3ES3ybLDY8y3ybHByzPfWtf7Dhj4av15J9lXVtrHnWM0izwaLPd8izwYn/3xu46UmjF1qYlFj3zn2AEexyLPBYs+3yLPBST7fQt5nlzS8RV3ZJQ3M2KUmFir2JNuTfDvJI0kW6rh2Sc5Nck+SQ0kOJLlh7JlWSrIpyf1Jvjz2LCslOSPJniQPT76Hl4090yuSfGzyM30oyW1JXjfyPHM5yOvCxJ5kE/C3wG8Cbwc+mOTt4071Ki8Bf1RVvwBcCvzBgs0HcANwaOwh1nATcHdV/TxwMQsyZ5KtwEeAbVV1EbAJuHbcqeZzkNeFiR14J/BIVT1aVS8Ct7N0YMuFUFVHqmr/5PMXWPrHunXcqf5PknOA9wK7xp5lpSSnA78O/CNAVb1YVT8adahX2wy8PslmYAvwxJjDzOsgr4sU+1bg8WVfH2aBYlouyXnAJcDekUdZ7kbgE8DLI8+xmguAZ4DPT+5m7Epy6thDAVTV94HPsnQQliPAc1X1tXGnWtWrDvIKrHmQ17UsUuyr/cL9wj0vmOQ04A7go1X1/NjzACS5Cni6qu4be5Y1bAbeAfx9VV0C/A8L8l4Dk/u+VwPnA2cDpya5btyp5mORYj8MnLvs63MYeTu1UpLXshT6rVV159jzLHM58L4k32Xp7s8VSW4Zd6RXOQwcrqpXdkJ7WIp/EbwH+E5VPVNVPwHuBN418kyrmfkgr4sU+73AhUnOT3IKSw+S3DXyTD+VJCzd5zxUVX899jzLVdWfVtU5VXUeS9+3f62qhVmdqupJ4PEkb5ucdCVwcMSRlnsMuDTJlsnP+EoW5MHDFWY+yOtG/4rrmqrqpSQfBr7K0iOiN1fVgZHHWu5y4LeBB5P8++S0T1XVV8Yb6YTyh8Ctk/+RPwpcP/I8AFTV3iR7gP0sPeNyPyO/bHb5QV6THGbpIK+fAb6Y5PeYHOT1uK/Xl8tKPSzSNl7SHBm71ISxS00Yu9SEsUtNGLvUhLFLTfwv8hceJxDRrUkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_data(train_dataset,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_data(train_dataset,N_images//2+2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can plot the 3rd  sample\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref3\"></a>\n",
    "\n",
    "### Build a Convolutional Neral Network Class\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input image is 11 x11, the following will change the size of the activations:\n",
    "\n",
    "<ul>\n",
    "<il>convolutional layer</il> \n",
    "</ul>\n",
    "<ul>\n",
    "<il>max pooling layer</il> \n",
    "</ul>\n",
    "<ul>\n",
    "<il>convolutional layer </il>\n",
    "</ul>\n",
    "<ul>\n",
    "<il>max pooling layer </il>\n",
    "</ul>\n",
    "\n",
    "with the following parameters <code>kernel_size</code>, <code>stride</code> and <code> pad</code>.\n",
    "We use the following  lines of code to change the image before we get tot he fully connected layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 10)\n",
      "(9, 9)\n",
      "(8, 8)\n",
      "(7, 7)\n"
     ]
    }
   ],
   "source": [
    "out=conv_output_shape((11,11), kernel_size=2, stride=1, pad=0, dilation=1)\n",
    "print(out)\n",
    "out1=conv_output_shape(out, kernel_size=2, stride=1, pad=0, dilation=1)\n",
    "print(out1)\n",
    "out2=conv_output_shape(out1, kernel_size=2, stride=1, pad=0, dilation=1)\n",
    "print(out2)\n",
    "\n",
    "out3=conv_output_shape(out2, kernel_size=2, stride=1, pad=0, dilation=1)\n",
    "print(out3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a Convolutional Network class with two Convolutional layers and one fully connected layer. Pre-determine the size of the final output matrix. The parameters in the constructor are the number of output channels for the first and second layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self,out_1=2,out_2=1):\n",
    "        \n",
    "        super(CNN,self).__init__()\n",
    "        #first Convolutional layers \n",
    "        self.cnn1=nn.Conv2d(in_channels=1,out_channels=out_1,kernel_size=2,padding=0)\n",
    "        self.maxpool1=nn.MaxPool2d(kernel_size=2 ,stride=1)\n",
    "\n",
    "        #second Convolutional layers\n",
    "        self.cnn2=nn.Conv2d(in_channels=out_1,out_channels=out_2,kernel_size=2,stride=1,padding=0)\n",
    "        self.maxpool2=nn.MaxPool2d(kernel_size=2 ,stride=1)\n",
    "        #max pooling \n",
    "\n",
    "        #fully connected layer \n",
    "        self.fc1=nn.Linear(out_2*7*7,2)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        #first Convolutional layers\n",
    "        x=self.cnn1(x)\n",
    "        #activation function \n",
    "        x=torch.relu(x)\n",
    "        #max pooling \n",
    "        x=self.maxpool1(x)\n",
    "        #first Convolutional layers\n",
    "        x=self.cnn2(x)\n",
    "        #activation function\n",
    "        x=torch.relu(x)\n",
    "        #max pooling\n",
    "        x=self.maxpool2(x)\n",
    "        #flatten output \n",
    "        x=x.view(x.size(0),-1)\n",
    "        #fully connected layer\n",
    "        x=self.fc1(x)\n",
    "        return x\n",
    "    \n",
    "    def activations(self,x):\n",
    "        #outputs activation this is not necessary just for fun \n",
    "        z1=self.cnn1(x)\n",
    "        a1=torch.relu(z1)\n",
    "        out=self.maxpool1(a1)\n",
    "        \n",
    "        z2=self.cnn2(out)\n",
    "        a2=torch.relu(z2)\n",
    "        out=self.maxpool2(a2)\n",
    "        out=out.view(out.size(0),-1)\n",
    "        return z1,a1,z2,a2,out        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ref3\"></a>\n",
    "\n",
    "<h2> Define the Convolutional Neral Network Classifier , Criterion function, Optimizer and Train the  Model  </h2> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 2 output channels for the first layer, and 1 outputs channel for the second layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=CNN(2,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see the model parameters with the object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (cnn1): Conv2d(1, 2, kernel_size=(2, 2), stride=(1, 1))\n",
       "  (maxpool1): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "  (cnn2): Conv2d(2, 1, kernel_size=(2, 2), stride=(1, 1))\n",
       "  (maxpool2): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=49, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the model parameters for the kernels before training the kernels. The kernels are initialized randomly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHkAAADrCAYAAABNVDkBAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAADu0lEQVR4nO3dPW4TURRA4Xv5kQJBwkVCQeMy7jMtYhnswItgKd4Fm6AyfdIgyki4cJGC7lLQBMkwGunNT3LO185IvqMjv5GleZ6sqtDT9mzuATQ+IwMYGcDIAEYGMDLAi74TMnMbEduIiPOzs+vNej36UBrux91dHI7HPHUsh/xO7jab2u92zQZTO912G/ubm5ORXa4BjAxgZAAjAxgZwMgARgYwMoCRAYwMYGQAIwMYGcDIAEYGMDKAkQGMDGBkACMDGBnAyABGBjAyQG/kzNxm5j4z9z+PxwlGUmu9katqV1VdVXWXq9UEI6k1l2sAIwMYGcDIAEYGMDKAkQGMDGBkACMDGBnAyABGBjAygJEBjAxgZAAjAxgZwMgARgYwMoCRAXpfT/DQt9v7yI9fx5plBp/mHqChl/88MugdFBGrRgNpSoN2UEScTzGTGvOeDGBkACMDGBnAyABGBjAygJEBjAxgZAAjAxgZwMgARgYwMoCRAYwMYGQAIwMYGcDIAEYGMDKAkQHcQQHgDgoAl2sAIwMYGcDIAEYGMDKAkQGMDGBkACMDGBnAyABGBjAygJEBjAxgZAAjAxgZwMgARgYwMoCRAbKq/n/CXw/Xx1VE3I49VERcRMRhgs+ZwlTXsq6qy1MHeiPPITP3fx7mf/yWcC0u1wBGBlhq5N3cAzQ0+7Us8p6stpb6TVZDRgYwMoCRAYwMMPA/Q55fR7wZeaQpDXqz8MLdR9WvPHVk0E+ozFVFfGg21vzezT1AQ1+i6nAysss1gJEBjAxgZAAjAxgZwMgARgYwMoCRAYwMYGQAIwMYGcDIAEYGMDKAkQGMDGBkACMDGBnAyABGBhi4g+LVyONoDO6geDLcQYFmZAAjAxgZwMgARgYwMoCRAYwMYGQAIwMYGcDIAEYGMDKAkQGMDGBkACMDGBnAyABGBjAywKDnrt9n1rb/tEdjM/cADX2OiO9VJ5+7HrSD4m3buTSR3uW6qnZV1VVV93qKidSc92QAIwMYGcDIAEYGMDKAkQGMDGBkACMDGBnAyABGBjAygJEBjAxgZAAjAxgZwMgARgYwMoCRAXy4HsCH6wFcrgGMDGBkACMDGBnAyABGBjAygJEBjAxgZAAjAxgZwMgARgYwMoCRAYwMYGQAIwMYGcDIAEYGMDJA7zsoHu6giIiriLgde6iIuIiIwwSfM4WprmVdVZenDgx60chUMnNfVd3cc7SwhGtxuQYwMsBSI+/mHqCh2a9lkfdktbXUb7IaMjKAkQGMDGBkgN9376R/Uv13JQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plot_channels(model.state_dict()['cnn1.weight'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAACqCAYAAACTZZUqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAEFUlEQVR4nO3asYpdVRiG4X/JFDISBsLYBNMkiBamMecybLyJfQfeh6VMmTvILZj2dE5pKwQiQ2wC2vwWWpzAxJ1tztlfzpzn6QZ2WB/D4iUsZnR3AbC+T9IDAE6VAAOECDBAiAADhAgwQIgAA4SczX0wxpiqavrnp0+fVn1x4El3yav0gCPzprr/GmuctHuvPzs/f/r1o0drHHsn/Hl9nZ5wVH6rqpvuW+/1WPJ3wGN82VU/7mnWKfgpPeDIvKju16sEeNfmyZPePn++9rFH69fHj9MTjsr3VfXLOwLsCQIgRIABQgQYIESAAUIEGCBEgAFCBBggRIABQgQYIESAAUIEGCBEgAFCBBggRIABQgQYIESAAUIEGCBEgAFCBBggRIABQgQYIESAAUIEGCBEgAFCBBggRIABQgQYIESAAUIEGCBEgAFCBBggRIABQgQYIESAAUIEGCBEgAFCZgM8xpjGGNsxxrbqjzU2wcHt3utXNzfpOZyo2QB391V3b7p7U3WxxiY4uN17/fn9++k5nChPEAAhAgwQIsAAIQIMECLAACECDBAiwAAhAgwQIsAAIQIMECLAACECDBAiwAAhAgwQIsAAIQIMECLAACECDBAiwAAhAgwQIsAAIQIMECLAACECDBAiwAAhAgwQIsAAIQIMECLAACECDBAiwAAhAgwQIsAAIQIMECLAACGju9/74wdj9HTAMXfNmx/e/3dL1bNnm3r5cjvWPvfbMfrntQ89Yvfqu/SEI/Oiul/feq/P5v7pGGOqqqmq6mLPsyBl914/DG/hdM0+QXT3VXdvuntzvsYiWMHuvb5Mj+FkeQMGCBFggBABBggRYIAQAQYIEWCAEAEGCBFggBABBggRYIAQAQYIEWCAEAEGCBFggBABBggRYIAQAQYIEWCAEAEGCBFggBABBggRYIAQAQYIEWCAEAEGCBFggBABBggRYIAQAQYIEWCAEAEGCBFggBABBggRYIAQAQYIEWCAkLO5D8YYU1VNVVUXB58D69i91w/DWzhds/8D7u6r7t509+Z8jUWwgt17fZkew8nyBAEQIsAAIQIMECLAACECDBAiwAAhAgwQIsAAIQIMECLAACECDBAiwAAhAgwQIsAAIQIMECLAACECDBAiwAAhAgwQIsAAIQIMECLAACECDBAiwAAhAgwQIsAAIQIMECLAACECDBAiwAAhAgwQIsAAIQIMECLAACECDBAiwAAho7v/+4Mxpqqa/v3xm6q6PvSo/+Gyqn5Pj7iFXct81d331jjIvf4gdi3zzns9G+C3Ph5j292bvc3aE7uWsevjOHeOXcsc4y5PEAAhAgwQsjTAVwdZ8eHsWsauj+PcOXYtc3S7Fr0BA7A/niAAQgQYIESAAUIEGCBEgAFC/gYhD8L7EAe/DgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_channels(model.state_dict()['cnn2.weight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the loss function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimizer class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "learning_rate=0.001\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the optimizer class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "train_loader=torch.utils.data.DataLoader(dataset=train_dataset,batch_size=10)\n",
    "validation_loader=torch.utils.data.DataLoader(dataset=validation_dataset,batch_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model and determine validation accuracy technically test accuracy **(This may take a long time)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_epochs=10\n",
    "cost_list=[]\n",
    "accuracy_list=[]\n",
    "N_test=len(validation_dataset)\n",
    "cost=0\n",
    "#n_epochs\n",
    "for epoch in range(n_epochs):\n",
    "    cost=0    \n",
    "    for x, y in train_loader:\n",
    "      \n",
    "\n",
    "        #clear gradient \n",
    "        optimizer.zero_grad()\n",
    "        #make a prediction \n",
    "        z=model(x)\n",
    "        # calculate loss \n",
    "        loss=criterion(z,y)\n",
    "        # calculate gradients of parameters \n",
    "        loss.backward()\n",
    "        # update parameters \n",
    "        optimizer.step()\n",
    "        cost+=loss.item()\n",
    "    cost_list.append(cost)\n",
    "        \n",
    "        \n",
    "    correct=0\n",
    "    #perform a prediction on the validation  data  \n",
    "    for x_test, y_test in validation_loader:\n",
    "\n",
    "        z=model(x_test)\n",
    "        _,yhat=torch.max(z.data,1)\n",
    "\n",
    "        correct+=(yhat==y_test).sum().item()\n",
    "\n",
    "    accuracy=correct/N_test\n",
    "\n",
    "    accuracy_list.append(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id=\"ref3\"></a>\n",
    "\n",
    "<h2 align=center>Analyse Results</h2> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the loss and accuracy on the validation data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "color = 'tab:red'\n",
    "ax1.plot(cost_list,color=color)\n",
    "ax1.set_xlabel('epoch',color=color)\n",
    "ax1.set_ylabel('total loss',color=color)\n",
    "ax1.tick_params(axis='y', color=color)\n",
    "    \n",
    "ax2 = ax1.twinx()  \n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('accuracy', color=color)  \n",
    "ax2.plot( accuracy_list, color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the results of the parameters for the Convolutional layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_dict()['cnn1.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_channels(model.state_dict()['cnn1.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.state_dict()['cnn1.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_channels(model.state_dict()['cnn2.weight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_data(train_dataset,N_images//2+2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the activations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out=model.activations(train_dataset[N_images//2+2][0].view(1,1,11,11))\n",
    "out=model.activations(train_dataset[0][0].view(1,1,11,11))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot them out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_activations(out[0],number_rows=1,name=\" feature map\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_activations(out[2],number_rows=1,name=\"2nd feature map\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_activations(out[3],number_rows=1,name=\"first feature map\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we save the output of the activation after flattening\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1=out[4][0].detach().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can do the same for a sample  where y=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out0=model.activations(train_dataset[100][0].view(1,1,11,11))[4][0].detach().numpy()\n",
    "out0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2, 1, 1)\n",
    "plt.plot( out1, 'b')\n",
    "plt.title('Flatted Activation Values  ')\n",
    "plt.ylabel('Activation')\n",
    "plt.xlabel('index')\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(out0, 'r')\n",
    "plt.xlabel('index')\n",
    "plt.ylabel('Activation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the Authors:\n",
    "\n",
    "[Joseph Santarcangelo](https://www.linkedin.com/in/joseph-s-50398b136/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0110ENSkillsNetwork20647811-2022-01-01) has a PhD in Electrical Engineering. His research focused on using machine learning, signal processing, and computer vision to determine how videos impact human cognition.\n",
    "\n",
    "Other contributors: [Michelle Carey](https://www.linkedin.com/in/michelleccarey/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0110ENSkillsNetwork20647811-2022-01-01)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Log\n",
    "\n",
    "| Date (YYYY-MM-DD) | Version | Changed By | Change Description                                          |\n",
    "| ----------------- | ------- | ---------- | ----------------------------------------------------------- |\n",
    "| 2020-09-23        | 2.0     | Srishti    | Migrated Lab to Markdown and added to course repo in GitLab |\n",
    "\n",
    "<hr>\n",
    "\n",
    "## <h3 align=\"center\"> Â© IBM Corporation 2020. All rights reserved. <h3/>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
